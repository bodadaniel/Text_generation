{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee675aa3-77a1-4f61-bd63-b363e65bc676",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipynb\n",
    "\n",
    "from ipynb.fs.full.LSTM_char import *\n",
    "from ipynb.fs.full.Data_import_preprocessing import *\n",
    "from ipynb.fs.full.Training import *\n",
    "from ipynb.fs.full.Evaluation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e8de401-36a1-4b30-8f2c-5e6340db19cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Info:\n",
      "    Info of relevant functions of global environment\n",
      "    \n",
      "    Inputs:\n",
      "        prefix_funcs(tuple): tuple of prefixes of functions in question\n",
      "\n",
      "    Outputs:\n",
      "        None, just print\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "def doc_funcs(prefix_funcs):\n",
    "    '''\n",
    "    Info:\n",
    "    Info of relevant functions of global environment\n",
    "    \n",
    "    Inputs:\n",
    "        prefix_funcs(tuple): tuple of prefixes of functions in question\n",
    "\n",
    "    Outputs:\n",
    "        None, just print\n",
    "    '''\n",
    "    funcs = [l for l in list(globals().keys()) if l.startswith(prefix_funcs)]\n",
    "    #print(*(print(func,':', globals()[func].__doc__) for func in funcs), sep = '\\n')\n",
    "    for func in funcs:\n",
    "        print(func, 'func:\\n', globals()[func].__doc__)\n",
    "\n",
    "print(doc_funcs.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0379813c-7b7c-4339-9bfa-8280ae53e8cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lc_CharModel func:\n",
      " char-based LSTM model\n",
      "    Input\n",
      "    n-vocab: constant - number of vocab \n",
      "    Output \n",
      "    nn model\n",
      "    \n",
      "lc_CharEmbModel func:\n",
      " char-based LSTM model with embedding input vectors: not ok yet\n",
      "    Input\n",
      "    n-vocab: constant - number of vocab\n",
      "    embedding_dim: constant - dimension of embedding vector\n",
      "    Output\n",
      "    nn model\n",
      "    \n",
      "lc_load_model func:\n",
      " load model and char_to_int dict\n",
      "    Inputs\n",
      "    model: obj - nn model\n",
      "    model_path: string - file path\n",
      "    Output\n",
      "    model obj, character to integer dict \n",
      "    \n",
      "dip_load_data func:\n",
      " import data from file and make the text lower case\n",
      "    Input\n",
      "    f_path: string - file path\n",
      "    f_name: string - file name\n",
      "    Output\n",
      "    raw text: string \n",
      "    \n",
      "dip_chars_dict func:\n",
      " set of chars with the corresponding mapping dicts\n",
      "    Input\n",
      "    text: sting - raw text\n",
      "    Output\n",
      "    character: string, character to integer: dict, integer to character: dict\n",
      "    \n",
      "dip_create_data func:\n",
      " create design matrix and response vector from raw text using specific window size\n",
      "    Input\n",
      "    text: string - raw text\n",
      "    chars: string - characters\n",
      "    char_to_int: dict - mapping from character to integer\n",
      "    seq_length: int - sequence length (example: XXXY/First three training/fourth label => seq_length = 3) \n",
      "    verbose: bool - print\n",
      "    Output\n",
      "    X: 2d list - training sample, y: list - label vector, number of pattern: int - example above equal one, number of vocab: int\n",
      "    \n",
      "dip_normalization_reshape func:\n",
      " create stensors, normalize X with number of vocab\n",
      "    Input\n",
      "    X: 2d list - training data \n",
      "    y: list - label vector \n",
      "    n_vocab: int - number of vocab\n",
      "    seq_length: int - sequence length (example: XXXY/First three training/fourth label => seq_length = 3) \n",
      "    verbose: bool - print \n",
      "    Output\n",
      "    X: 3d tensor: training data shape of (batch size/sequence length/1), y: 1d tensor: label vector\n",
      "    \n",
      "tr_device_fn func:\n",
      " info of GPU support\n",
      "    Input\n",
      "    GPU: bool - GPU support\n",
      "    Output\n",
      "    device: obj\n",
      "    \n",
      "tr_training func:\n",
      " Train the model\n",
      "    Input\n",
      "    X: tensor - training data shape of (batch size/sequence length/1)\n",
      "    y: tensor - response vector\n",
      "    model: obj - nn model\n",
      "    optimizer: nn optimizer\n",
      "    loss_fn: nn loss funciton\n",
      "    char_to_int: dict - character to integer mapping\n",
      "    n_epochs: number of epoch\n",
      "    batch_size: batch size\n",
      "    device: device cpu or mps\n",
      "    save: save model\n",
      "    verbose: print\n",
      "    Output\n",
      "    model: obj\n",
      "    \n",
      "ev_text_gen_prompt func:\n",
      " generate random prompt text\n",
      "    Input\n",
      "    text: string - raw text\n",
      "    char_to_int: dict - character to integer mapping\n",
      "    seq_len: int - sequence length (example: XXXY/First three training/fourth label => seq_length = 3) \n",
      "    verbose: bool - print \n",
      "    Output\n",
      "    prompt: str, prompt: list - numeric form \n",
      "    \n",
      "ev_eval func:\n",
      " evaluation by writing out the generated text starting from prompt\n",
      "    Input\n",
      "    X: list - numeric form of prompt\n",
      "    prompt: string - text input\n",
      "    n_text: int - number of generated character\n",
      "    n_vocab: int - size of vocab\n",
      "    model: obj - nn model\n",
      "    device: obj - device\n",
      "    load_model: none\n",
      "    verbose: bool - print \n",
      "    Output\n",
      "    result: list - generated text \n",
      "    \n"
     ]
    }
   ],
   "source": [
    "doc_funcs(prefix_funcs = ('dip', 'lc', 'tr', 'ev'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "074f5734-9f4c-4ee3-98aa-2698d23aee5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 100\n",
    "n_epochs = 2\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "073554ac-1b89-4606-8b26-6b99a881b532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total Characters: \n",
      "144059\n",
      "Total Vocab: \n",
      "47\n",
      "Total Patterns: \n",
      "143959\n",
      "\n",
      "\n",
      "\n",
      "Size of X: torch.Size([143959, 100, 1]) Size of y: torch.Size([143959]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = dip_load_data('/Users/danielboda', 'wonderland.txt')\n",
    "chars, char_to_int, int_to_char = dip_chars_dict(text)\n",
    "X, y, n_patterns, n_vocab = dip_create_data(text, chars, char_to_int, verbose = True)   \n",
    "X, y = dip_normalization_reshape(X, y, n_patterns, n_vocab, seq_length, verbose = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ee7e19d-52d4-4521-9c87-3684111a3674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "start: 2023-11-20 12:56:05.821909 \n",
      "\n",
      "Epoch 1: Cross-entropy: 370025.5000\n",
      "estimated end: 2023-11-20 15:57:51.368469\n",
      "Epoch 2: Cross-entropy: 329787.4375\n",
      "estimated end: 2023-11-20 15:57:31.057016\n",
      "Epoch 3: Cross-entropy: 302646.7188\n",
      "estimated end: 2023-11-20 15:55:30.787412\n",
      "Epoch 4: Cross-entropy: 290776.0625\n",
      "estimated end: 2023-11-20 15:55:25.070162\n",
      "Epoch 5: Cross-entropy: 273362.7500\n",
      "estimated end: 2023-11-20 15:58:20.636242\n",
      "Epoch 6: Cross-entropy: 262139.5469\n",
      "estimated end: 2023-11-20 15:58:23.722942\n",
      "Epoch 7: Cross-entropy: 254904.4062\n",
      "estimated end: 2023-11-20 15:57:14.592290\n",
      "Epoch 8: Cross-entropy: 243251.9062\n",
      "estimated end: 2023-11-20 15:57:04.275346\n",
      "Epoch 9: Cross-entropy: 237150.5781\n",
      "estimated end: 2023-11-20 15:57:21.806554\n",
      "Epoch 10: Cross-entropy: 231861.6250\n",
      "estimated end: 2023-11-20 15:57:49.031078\n",
      "Epoch 11: Cross-entropy: 224011.0312\n",
      "estimated end: 2023-11-20 15:56:35.195568\n",
      "Epoch 12: Cross-entropy: 220259.8594\n",
      "estimated end: 2023-11-20 15:57:46.551021\n",
      "Epoch 13: Cross-entropy: 213074.5938\n",
      "estimated end: 2023-11-20 15:57:21.021509\n",
      "Epoch 14: Cross-entropy: 207488.7500\n",
      "estimated end: 2023-11-20 15:59:56.503049\n",
      "Epoch 15: Cross-entropy: 204131.7031\n",
      "estimated end: 2023-11-20 15:57:47.015207\n",
      "Epoch 16: Cross-entropy: 200116.4531\n",
      "estimated end: 2023-11-20 16:16:32.582377\n",
      "Epoch 17: Cross-entropy: 195377.5469\n",
      "estimated end: 2023-11-20 16:12:23.689961\n",
      "Epoch 18: Cross-entropy: 192948.4844\n",
      "estimated end: 2023-11-20 16:21:05.201315\n",
      "Epoch 19: Cross-entropy: 187408.1875\n",
      "estimated end: 2023-11-20 16:31:51.630303\n",
      "Epoch 20: Cross-entropy: 183992.8750\n",
      "estimated end: 2023-11-20 16:30:55.141131\n",
      "Epoch 21: Cross-entropy: 183111.2031\n",
      "estimated end: 2023-11-21 04:46:31.989951\n",
      "Epoch 22: Cross-entropy: 180533.7188\n",
      "estimated end: 2023-11-20 16:12:20.588084\n",
      "Epoch 23: Cross-entropy: 172402.9219\n",
      "estimated end: 2023-11-20 16:14:36.824980\n",
      "Epoch 24: Cross-entropy: 168563.2969\n",
      "estimated end: 2023-11-20 16:11:49.146484\n",
      "Epoch 25: Cross-entropy: 166037.2969\n",
      "estimated end: 2023-11-20 16:12:10.437460\n",
      "Epoch 26: Cross-entropy: 162913.6719\n",
      "estimated end: 2023-11-20 16:12:12.279190\n",
      "Epoch 27: Cross-entropy: 160494.0000\n",
      "estimated end: 2023-11-20 16:12:44.680540\n",
      "Epoch 28: Cross-entropy: 158187.3750\n",
      "estimated end: 2023-11-20 16:18:08.652103\n",
      "Epoch 29: Cross-entropy: 155472.5469\n",
      "estimated end: 2023-11-20 16:17:02.079623\n",
      "Epoch 30: Cross-entropy: 151732.6406\n",
      "estimated end: 2023-11-20 16:23:51.527156\n",
      "Epoch 31: Cross-entropy: 150686.0000\n",
      "estimated end: 2023-11-20 16:25:54.457806\n",
      "Epoch 32: Cross-entropy: 147072.7812\n",
      "estimated end: 2023-11-20 16:33:09.860389\n",
      "Epoch 33: Cross-entropy: 145617.3281\n",
      "estimated end: 2023-11-20 16:20:42.284821\n",
      "Epoch 34: Cross-entropy: 143434.4219\n",
      "estimated end: 2023-11-20 16:16:56.664846\n",
      "Epoch 35: Cross-entropy: 139149.6094\n",
      "estimated end: 2023-11-20 16:22:56.995128\n",
      "Epoch 36: Cross-entropy: 138761.5156\n",
      "estimated end: 2023-11-20 16:21:18.411018\n",
      "Epoch 37: Cross-entropy: 134571.5625\n",
      "estimated end: 2023-11-20 16:23:34.257630\n",
      "Epoch 38: Cross-entropy: 136979.2188\n",
      "estimated end: 2023-11-20 16:22:02.243478\n",
      "Epoch 39: Cross-entropy: 131817.0469\n",
      "estimated end: 2023-11-20 16:22:19.568982\n",
      "Epoch 40: Cross-entropy: 129373.1641\n",
      "estimated end: 2023-11-20 16:25:54.250886\n",
      "Epoch 41: Cross-entropy: 130036.6719\n",
      "estimated end: 2023-11-20 16:24:11.969926\n",
      "Epoch 42: Cross-entropy: 126745.6250\n",
      "estimated end: 2023-11-20 16:23:00.777337\n",
      "Epoch 43: Cross-entropy: 124240.9844\n",
      "estimated end: 2023-11-20 16:23:53.728969\n",
      "Epoch 44: Cross-entropy: 124007.5391\n",
      "estimated end: 2023-11-20 16:24:24.721168\n",
      "Epoch 45: Cross-entropy: 121284.8984\n",
      "estimated end: 2023-11-20 16:22:22.409512\n",
      "Epoch 46: Cross-entropy: 122188.5938\n",
      "estimated end: 2023-11-20 16:22:59.053462\n",
      "Epoch 47: Cross-entropy: 117492.8438\n",
      "estimated end: 2023-11-20 16:23:41.045128\n",
      "Epoch 48: Cross-entropy: 117825.9922\n",
      "estimated end: 2023-11-20 16:24:30.115072\n",
      "Epoch 49: Cross-entropy: 113604.7812\n",
      "estimated end: 2023-11-20 16:24:32.457888\n",
      "Epoch 50: Cross-entropy: 115434.7266\n",
      "estimated end: 2023-11-20 16:24:29.591783\n",
      "Epoch 51: Cross-entropy: 112306.5469\n",
      "estimated end: 2023-11-20 16:26:15.970193\n",
      "Epoch 52: Cross-entropy: 111146.2344\n",
      "estimated end: 2023-11-20 16:25:15.602894\n",
      "Epoch 53: Cross-entropy: 107562.1875\n",
      "estimated end: 2023-11-20 16:25:26.362398\n",
      "Epoch 54: Cross-entropy: 109492.0703\n",
      "estimated end: 2023-11-20 16:26:12.712866\n",
      "Epoch 55: Cross-entropy: 105323.7891\n",
      "estimated end: 2023-11-20 16:25:43.866828\n",
      "Epoch 56: Cross-entropy: 106858.5859\n",
      "estimated end: 2023-11-20 16:24:49.457253\n",
      "Epoch 57: Cross-entropy: 105746.6016\n",
      "estimated end: 2023-11-20 16:25:16.061757\n",
      "Epoch 58: Cross-entropy: 101790.5547\n",
      "estimated end: 2023-11-20 16:25:07.418702\n",
      "Epoch 59: Cross-entropy: 100772.3203\n",
      "estimated end: 2023-11-20 16:25:33.003976\n",
      "Epoch 60: Cross-entropy: 101434.7734\n",
      "estimated end: 2023-11-20 16:25:55.813063\n",
      "Epoch 61: Cross-entropy: 100398.7891\n",
      "estimated end: 2023-11-20 16:25:27.786283\n",
      "Epoch 62: Cross-entropy: 101532.3047\n",
      "estimated end: 2023-11-20 16:24:47.514871\n",
      "Epoch 63: Cross-entropy: 94244.2656\n",
      "estimated end: 2023-11-20 16:26:01.554235\n",
      "Epoch 64: Cross-entropy: 97220.5938\n",
      "estimated end: 2023-11-20 16:25:46.702338\n",
      "Epoch 65: Cross-entropy: 95435.0781\n",
      "estimated end: 2023-11-20 16:25:13.817890\n",
      "Epoch 66: Cross-entropy: 95329.1016\n",
      "estimated end: 2023-11-20 16:25:40.040080\n",
      "Epoch 67: Cross-entropy: 92753.9609\n",
      "estimated end: 2023-11-20 16:25:29.456920\n",
      "Epoch 68: Cross-entropy: 92418.5000\n",
      "estimated end: 2023-11-20 16:25:19.936318\n",
      "Epoch 69: Cross-entropy: 96183.0000\n",
      "estimated end: 2023-11-20 16:27:35.837734\n",
      "Epoch 70: Cross-entropy: 91041.7734\n",
      "estimated end: 2023-11-20 16:26:44.477766\n",
      "Epoch 71: Cross-entropy: 87800.8828\n",
      "estimated end: 2023-11-20 16:25:38.010146\n",
      "Epoch 72: Cross-entropy: 90502.9297\n",
      "estimated end: 2023-11-20 16:27:06.321422\n",
      "Epoch 73: Cross-entropy: 90199.5469\n",
      "estimated end: 2023-11-20 16:28:46.601102\n",
      "Epoch 74: Cross-entropy: 87540.0469\n",
      "estimated end: 2023-11-20 16:31:52.069875\n",
      "Epoch 75: Cross-entropy: 87899.8281\n",
      "estimated end: 2023-11-20 16:30:39.301767\n",
      "Epoch 76: Cross-entropy: 86740.5781\n",
      "estimated end: 2023-11-20 16:30:19.376397\n",
      "Epoch 77: Cross-entropy: 86056.4922\n",
      "estimated end: 2023-11-20 16:29:40.142985\n",
      "Epoch 78: Cross-entropy: 87032.3672\n",
      "estimated end: 2023-11-20 16:28:12.496848\n",
      "Epoch 79: Cross-entropy: 85516.9375\n",
      "estimated end: 2023-11-20 16:28:22.113144\n",
      "Epoch 80: Cross-entropy: 85682.8047\n",
      "estimated end: 2023-11-20 16:28:21.248855\n",
      "\n",
      "end: 2023-11-20 16:28:21.248941 \n",
      " \n",
      "\n",
      "\n",
      "Model(weights + char_to_int) saved as: lc_CharModel_1700494101.pth in directory: /Users/danielboda/Text_generation \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = lc_CharModel(n_vocab)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "loss_fn = nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "device = tr_device_fn(GPU = True)\n",
    "model = tr_training(X, y, model, optimizer, loss_fn, char_to_int, n_epochs, batch_size, device, save = True, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7dbd5fa3-fdb9-449f-9159-51940b7ea92e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: \"w,” the mock turtle went on, “you throw the—”\n",
      "\n",
      "“the lobsters!” shouted the gryphon, with a bound int\"\n",
      "Generated: \n",
      "o the soo of the whodow, and she found the little great lust as she spoke to the dormouse to gave the same some cefo of shener. “what a pity yes ” said the caterpillar.\n",
      "\n",
      "“well, things is the capetal of percam of things!” alice seiled the supence the court, and snok alice looked all tooethies, and the words came ruite sound her arm ant off, and was nowing to find the same again.\n",
      "\n",
      "“i don’t much mittle puite must be ” thought alice. \n",
      "“well, i’ve often lever then to be a coomous in yourself,” the mock turtle replied; “and then, the dam tale to diset of meaning about ier and diries, i think the leaves, and the same shgne, and that’s all it  i’ll try in my life!”\n",
      "\n",
      "she had not the same was nore as all the crows goaat and stope to do so hale tadly, the other queen seilid a little of the ouher bu herself foo the cank, with a suunytily it to be then, and then all the whole party whth them, and then all the crowsed sur that the had nothing gamlen that she had to out her hand again, and the grypho\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'o'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt, prompt_int = ev_text_gen_prompt(text, char_to_int, seq_length, verbose = False)\n",
    "ev_eval(prompt_int, prompt, 1000, n_vocab, model, device, load_model = None, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86f389dc-9948-432f-b1fd-8c4183c0a8b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "start: 2023-11-21 08:59:38.475352 \n",
      "\n",
      "Epoch 1: Cross-entropy: 441540.4062\n",
      "estimated end: 2023-11-21 09:03:57.494568\n",
      "Epoch 2: Cross-entropy: 441244.4375\n",
      "estimated end: 2023-11-21 09:04:00.152908\n",
      "\n",
      "end: 2023-11-21 09:04:00.153035 \n",
      " \n",
      "\n",
      "\n",
      "Model(weights + char_to_int) saved as: lc_CharEmbModel_1700553840.pth in directory: /Users/danielboda/Text_generation \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = lc_CharEmbModel(n_vocab)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "loss_fn = nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "device = tr_device_fn(GPU = True)\n",
    "model = tr_training(X, y, model, optimizer, loss_fn, char_to_int, n_epochs, batch_size, device, save = True, verbose = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
